# æŠ€æœ¯ç ”ç©¶æŠ¥å‘Šï¼šPostgreSQL è‡ªç„¶è¯­è¨€æŸ¥è¯¢ MCP æœåŠ¡å™¨

**æ—¥æœŸ**: 2026-01-28
**çŠ¶æ€**: Phase 0 å®Œæˆ âœ… | **å®æ–½**: Phase 3 å®Œæˆ ğŸš€
**ç›¸å…³è®¡åˆ’**: [plan.md](./plan.md)
**æ¢ç´¢ææ–™**: [explore/](./explore/README.md)ï¼ˆ22 ä¸ªæ–‡ä»¶ï¼Œ275KB è¯¦ç»†ç ”ç©¶ï¼‰

æœ¬æ–‡æ¡£æ•´åˆäº†æ‰€æœ‰æŠ€æœ¯ç ”ç©¶çš„å…³é”®å‘ç°ã€è®¾è®¡å†³ç­–å’Œå®ç°å»ºè®®ï¼Œä¸ºåç»­å¼€å‘æä¾›æŠ€æœ¯ä¾æ®ã€‚

---

## ç›®å½•

1. [ç ”ç©¶æ¦‚è¿°](#1-ç ”ç©¶æ¦‚è¿°)
2. [FastMCP é›†æˆæ¨¡å¼](#2-fastmcp-é›†æˆæ¨¡å¼)
3. [Asyncpg è¿æ¥æ± æ¶æ„](#3-asyncpg-è¿æ¥æ± æ¶æ„)
4. [SQLGlot SQL å®‰å…¨éªŒè¯](#4-sqlglot-sql-å®‰å…¨éªŒè¯)
5. [OpenAI Prompt Engineering](#5-openai-prompt-engineering)
6. [Pydantic v2 æ•°æ®æ¨¡å‹](#6-pydantic-v2-æ•°æ®æ¨¡å‹)
7. [æŸ¥è¯¢æ¨¡æ¿åº“è®¾è®¡](#7-æŸ¥è¯¢æ¨¡æ¿åº“è®¾è®¡)
8. [JSONL æ—¥å¿—ç³»ç»Ÿ](#8-jsonl-æ—¥å¿—ç³»ç»Ÿ)
9. [æŠ€æœ¯å†³ç­–è¡¨](#9-æŠ€æœ¯å†³ç­–è¡¨)
10. [é£é™©ä¸ç¼“è§£](#10-é£é™©ä¸ç¼“è§£)

---

## 1. ç ”ç©¶æ¦‚è¿°

### 1.1 ç ”ç©¶ç›®æ ‡

ä¸º PostgreSQL è‡ªç„¶è¯­è¨€æŸ¥è¯¢ MCP æœåŠ¡å™¨é€‰æ‹©æœ€ä¼˜æŠ€æœ¯æ ˆï¼ŒéªŒè¯æŠ€æœ¯å¯è¡Œæ€§ï¼Œå¹¶ä¸ºå®ç°æä¾›è¯¦ç»†çš„æ¶æ„è®¾è®¡å’Œä»£ç æ¨¡å¼ã€‚

### 1.2 ç ”ç©¶æ–¹æ³•

- **å¹¶è¡Œç ”ç©¶**: 4 ä¸ªç‹¬ç«‹ç ”ç©¶ä»£ç†åŒæ—¶å·¥ä½œ
- **æ·±åº¦åˆ†æ**: æ¯ä¸ªä¸»é¢˜ 1000+ è¡Œè¯¦ç»†æ–‡æ¡£
- **åŸå‹éªŒè¯**: 3,800+ è¡Œç¤ºä¾‹ä»£ç å’Œæµ‹è¯•
- **æ€§èƒ½åŸºå‡†**: å®é™…æµ‹è¯•éªŒè¯æ€§èƒ½æŒ‡æ ‡

### 1.3 ç ”ç©¶æˆæœ

| ä¸»é¢˜ | æ–‡æ¡£ | ä»£ç  | çŠ¶æ€ |
|------|------|------|------|
| FastMCP é›†æˆ | 46KB | 200 LOC | âœ… å®Œæˆ |
| Asyncpg è¿æ¥æ±  | 48KB | 1,100 LOC | âœ… å®Œæˆ |
| SQLGlot éªŒè¯ | 32KB | 1,450 LOC | âœ… å®Œæˆ |
| OpenAI Prompt | 44KB | 400 LOC | âœ… å®Œæˆ |
| æ¨¡æ¿åº“è®¾è®¡ | 65KB | 650 LOC | âœ… å®Œæˆ |

**æ€»è®¡**: 235KB æ–‡æ¡£ + 3,800 LOC ç¤ºä¾‹ä»£ç 

---

## 2. FastMCP é›†æˆæ¨¡å¼

### 2.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: ä½¿ç”¨ FastMCP 0.3+ ä½œä¸º MCP æœåŠ¡å™¨æ¡†æ¶

**ç†ç”±**:
- âœ… å£°æ˜å¼ API å‡å°‘ 80% æ ·æ¿ä»£ç 
- âœ… Pydantic è‡ªåŠ¨å‚æ•°éªŒè¯å’Œ JSON Schema ç”Ÿæˆ
- âœ… å¼‚æ­¥åŸç”Ÿï¼Œä¸ Asyncpg æ— ç¼é›†æˆ
- âœ… å†…ç½®é”™è¯¯å¤„ç†ï¼ˆToolErrorï¼‰

**æ›¿ä»£æ–¹æ¡ˆ**: åŸç”Ÿ MCP SDK - éœ€è¦ 300+ è¡Œé¢å¤–ä»£ç ï¼Œä¸æ¨è

### 2.2 æ ¸å¿ƒæ¨¡å¼ï¼šLifespan ç®¡ç†

**æ¨¡å¼**: ä½¿ç”¨ `@asynccontextmanager` ç®¡ç†æœåŠ¡å™¨ç”Ÿå‘½å‘¨æœŸ

```python
from contextlib import asynccontextmanager
from dataclasses import dataclass

@dataclass
class ServerContext:
    """æœåŠ¡å™¨å…±äº«çŠ¶æ€"""
    pool_manager: PoolManager
    schema_cache: SchemaCache
    sql_generator: SQLGenerator
    config: Config

@asynccontextmanager
async def lifespan():
    """æœåŠ¡å™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # Startup: åˆå§‹åŒ–èµ„æº
    config = Config.load()
    pool_manager = PoolManager(config.databases)
    await pool_manager.initialize()  # åˆ›å»ºæ‰€æœ‰è¿æ¥æ± 

    schema_cache = SchemaCache(pool_manager)
    await schema_cache.load_all()  # ç¼“å­˜æ‰€æœ‰ schemas

    sql_generator = SQLGenerator(schema_cache, config.openai)

    context = ServerContext(
        pool_manager=pool_manager,
        schema_cache=schema_cache,
        sql_generator=sql_generator,
        config=config
    )

    try:
        yield context  # æä¾›ç»™æ‰€æœ‰å·¥å…·ä½¿ç”¨
    finally:
        # Shutdown: æ¸…ç†èµ„æº
        await pool_manager.close_all()
        logger.info("æ‰€æœ‰è¿æ¥æ± å·²å…³é—­")
```

**å…³é”®ç‚¹**:
- è¿æ¥æ± åªåˆå§‹åŒ–ä¸€æ¬¡ï¼ˆå¯åŠ¨æ—¶ï¼‰
- æ‰€æœ‰å·¥å…·å…±äº«ç›¸åŒçš„æ± å’Œç¼“å­˜
- ä¼˜é›…å…³é—­ç¡®ä¿è¿æ¥æ­£ç¡®é‡Šæ”¾

### 2.3 å·¥å…·å®šä¹‰æ¨¡å¼

**æ¨¡å¼**: åŸºäº Pydantic æ¨¡å‹çš„å·¥å…·å®šä¹‰

```python
from fastmcp import FastMCP, Context, ToolError
from pydantic import BaseModel, Field

mcp = FastMCP("postgres-mcp", lifespan=lifespan)

class GenerateSQLInput(BaseModel):
    """generate_sql å·¥å…·è¾“å…¥"""
    natural_language: str = Field(
        ...,
        description="è‡ªç„¶è¯­è¨€æŸ¥è¯¢æè¿°",
        min_length=1,
        max_length=2000
    )
    database: str | None = Field(
        None,
        description="ç›®æ ‡æ•°æ®åº“åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„é»˜è®¤æ•°æ®åº“ï¼‰"
    )

@mcp.tool()
async def generate_sql(input: GenerateSQLInput, ctx: Context) -> dict:
    """æ ¹æ®è‡ªç„¶è¯­è¨€ç”Ÿæˆ SQL æŸ¥è¯¢ï¼ˆä¸æ‰§è¡Œï¼‰"""
    try:
        # è®¿é—®å…±äº«çŠ¶æ€
        context: ServerContext = ctx.request_context.lifespan_context

        # ç¡®å®šæ•°æ®åº“
        db_name = input.database or context.config.default_database

        # ç”Ÿæˆ SQL
        result = await context.sql_generator.generate(
            natural_language=input.natural_language,
            database=db_name
        )

        return {
            "sql": result.sql,
            "validated": result.validated,
            "warnings": result.warnings,
            "explanation": result.explanation,
            "generation_method": result.generation_method
        }

    except DatabaseNotFoundError as e:
        raise ToolError(f"æ•°æ®åº“ '{input.database}' ä¸å­˜åœ¨æˆ–æœªé…ç½®") from e
    except AIServiceUnavailableError as e:
        raise ToolError("AI æœåŠ¡å½“å‰ä¸å¯ç”¨ï¼Œå·²å°è¯•æ¨¡æ¿åŒ¹é…") from e
    except Exception as e:
        logger.exception("ç”Ÿæˆ SQL æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯")
        raise ToolError(f"å†…éƒ¨é”™è¯¯: {str(e)}") from e
```

### 2.4 èµ„æºæš´éœ²æ¨¡å¼

**æ¨¡å¼**: åŠ¨æ€ URI æ¨¡æ¿èµ„æº

```python
@mcp.resource("schema://{database}")
async def get_database_schema(uri: str, ctx: Context) -> str:
    """è¿”å›æ•°æ®åº“ schemaï¼ˆMCP èµ„æºï¼‰"""
    # è§£æ URI
    database = uri.split("://")[1]

    # è®¿é—®ç¼“å­˜
    context: ServerContext = ctx.request_context.lifespan_context
    schema = await context.schema_cache.get_schema(database)

    if not schema:
        raise ToolError(f"æ•°æ®åº“ '{database}' çš„ schema æœªç¼“å­˜")

    # è¿”å› JSON æ ¼å¼
    return schema.to_json()

@mcp.resource("schema://{database}/{table}")
async def get_table_schema(uri: str, ctx: Context) -> str:
    """è¿”å›è¡¨ schemaï¼ˆMCP èµ„æºï¼‰"""
    parts = uri.split("://")[1].split("/")
    database, table = parts[0], parts[1]

    context: ServerContext = ctx.request_context.lifespan_context
    schema = await context.schema_cache.get_schema(database)

    if not schema or table not in schema.tables:
        raise ToolError(f"è¡¨ '{table}' ä¸å­˜åœ¨äºæ•°æ®åº“ '{database}'")

    return schema.tables[table].model_dump_json(indent=2)
```

### 2.5 å…³é”®é™·é˜±

âš ï¸ **é™·é˜± 1: Pydantic åµŒå¥—æ¨¡å‹åºåˆ—åŒ–**
- **é—®é¢˜**: LLM å¯èƒ½å‘é€å­—ç¬¦ä¸²åŒ– JSON è€Œéå¯¹è±¡
- **è§£å†³**: ä½¿ç”¨æ‰å¹³å‚æ•°æˆ–æ·»åŠ è‡ªå®šä¹‰ validator

âš ï¸ **é™·é˜± 2: è¿æ¥æ± æ³„æ¼**
- **é—®é¢˜**: æ‰‹åŠ¨ `acquire()`/`release()` æ˜“å¿˜è®°
- **è§£å†³**: å§‹ç»ˆä½¿ç”¨ `async with pool.acquire() as conn:`

âš ï¸ **é™·é˜± 3: ä¸Šä¸‹æ–‡ä¼ é€’é—®é¢˜**
- **é—®é¢˜**: StreamableHTTP transport å¯èƒ½æœ‰ä¸Šä¸‹æ–‡ä¸¢å¤±
- **è§£å†³**: ä½¿ç”¨ stdio transportï¼ˆMCP æ ‡å‡†ï¼‰

**è¯¦ç»†æ–‡æ¡£**: `explore/fastmcp_research.md` (46KB, 20+ ä»£ç ç¤ºä¾‹)

---

## 3. Asyncpg è¿æ¥æ± æ¶æ„

### 3.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: æ¯æ•°æ®åº“ç‹¬ç«‹è¿æ¥æ± æ¶æ„

**ç†ç”±**:
- âœ… Asyncpg ä¸æ”¯æŒåŒä¸€è¿æ¥åˆ‡æ¢æ•°æ®åº“
- âœ… æ€§èƒ½æœ€ä¼˜ï¼ˆæ¯” SQLAlchemy Async å¿« 2-3 å€ï¼‰
- âœ… å†…ç½®è¿æ¥æ± ï¼ˆ`asyncpg.create_pool()`ï¼‰
- âœ… è‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œè¿æ¥å›æ”¶

**æ›¿ä»£æ–¹æ¡ˆ**: SQLAlchemy 2.0 Async - å¢åŠ  ORM å¤æ‚åº¦ï¼Œæœ¬é¡¹ç›®ä¸éœ€è¦

### 3.2 è¿æ¥æ± é…ç½®

**æ¨èé…ç½®**ï¼ˆ10+ å¹¶å‘åœºæ™¯ï¼‰:

```python
pool_config = {
    "min_size": 5,        # åŸºçº¿è¿æ¥æ•°ï¼ˆå§‹ç»ˆä¿æŒï¼‰
    "max_size": 20,       # å³°å€¼å¹¶å‘è´Ÿè½½
    "max_queries": 50000, # 50k æŸ¥è¯¢åå›æ”¶è¿æ¥
    "max_inactive_connection_lifetime": 300.0,  # 5 åˆ†é’Ÿç©ºé—²è‡ªåŠ¨å…³é—­
    "command_timeout": 60.0,  # å®¢æˆ·ç«¯å‘½ä»¤è¶…æ—¶
    "server_settings": {
        "statement_timeout": "30000",     # 30 ç§’æŸ¥è¯¢è¶…æ—¶
        "idle_in_transaction_session_timeout": "60000",  # 60 ç§’äº‹åŠ¡è¶…æ—¶
    }
}
```

**é…ç½®è®¡ç®—å…¬å¼**:

```python
å¹¶å‘æŸ¥è¯¢æ•° = 10
æ•°æ®åº“æ•°é‡ = 5
å®‰å…¨ç³»æ•° = 2

min_size = max(2, å¹¶å‘æŸ¥è¯¢æ•° // æ•°æ®åº“æ•°é‡) = 5
max_size = min_size * å®‰å…¨ç³»æ•° = 20
```

### 3.3 PoolManager æ¶æ„

**è®¾è®¡**: é›†ä¸­å¼è¿æ¥æ± ç®¡ç†å™¨

```python
from typing import Dict
import asyncpg
from pybreaker import CircuitBreaker

class PoolManager:
    """å¤šæ•°æ®åº“è¿æ¥æ± ç®¡ç†å™¨"""

    def __init__(self, db_configs: List[DBConfig]):
        self._pools: Dict[str, asyncpg.Pool] = {}
        self._configs = {cfg.name: cfg for cfg in db_configs}
        self._breakers: Dict[str, CircuitBreaker] = {}

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰é¢„é…ç½®çš„è¿æ¥æ± """
        tasks = [
            self._create_pool(name, config)
            for name, config in self._configs.items()
        ]
        await asyncio.gather(*tasks)
        logger.info(f"å·²åˆå§‹åŒ– {len(self._pools)} ä¸ªè¿æ¥æ± ")

    async def _create_pool(self, name: str, config: DBConfig):
        """åˆ›å»ºå•ä¸ªæ•°æ®åº“è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            host=config.host,
            port=config.port,
            database=config.database,
            user=config.user,
            password=os.getenv(config.password_env_var),
            min_size=config.min_pool_size,
            max_size=config.max_pool_size,
            **pool_config  # åº”ç”¨ä¸Šè¿°é…ç½®
        )

        self._pools[name] = pool
        self._breakers[name] = CircuitBreaker(
            fail_max=5,          # 5 æ¬¡å¤±è´¥åç†”æ–­
            timeout_duration=60  # 60 ç§’åé‡è¯•
        )

    async def get_connection(self, database: str):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        if database not in self._pools:
            raise DatabaseNotFoundError(f"æ•°æ®åº“ '{database}' æœªé…ç½®")

        pool = self._pools[database]
        breaker = self._breakers[database]

        # ç†”æ–­å™¨æ£€æŸ¥
        if breaker.current_state == 'open':
            raise PoolExhaustedError("è¿æ¥æ± ç†”æ–­ï¼ˆè¿‡è½½ä¿æŠ¤ï¼‰")

        try:
            async with pool.acquire() as conn:
                yield conn
                breaker.call_succeeded()
        except asyncpg.TooManyConnectionsError:
            breaker.call_failed()
            raise PoolExhaustedError("è¿æ¥æ± å·²æ»¡ï¼Œè¯·ç¨åé‡è¯•")
```

### 3.4 è¶…æ—¶é…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ | è§¦å‘è¡Œä¸º |
|------|-----|------|---------|
| `statement_timeout` | 30000ms | å•ä¸ªæŸ¥è¯¢æœ€å¤§æ‰§è¡Œæ—¶é—´ | å–æ¶ˆæŸ¥è¯¢ï¼Œå›æ»š |
| `idle_in_transaction_session_timeout` | 60000ms | äº‹åŠ¡å†…ç©ºé—²è¶…æ—¶ | ç»ˆæ­¢ä¼šè¯ |
| `command_timeout` | 60.0s | Asyncpg å®¢æˆ·ç«¯è¶…æ—¶ | æŠ›å‡º TimeoutError |
| `max_inactive_connection_lifetime` | 300.0s | è¿æ¥ç©ºé—²è‡ªåŠ¨å…³é—­ | å…³é—­å¹¶é‡å»ºè¿æ¥ |

### 3.5 é”™è¯¯å¤„ç†ç­–ç•¥

| é”™è¯¯ç±»å‹ | é‡è¯• | ç†”æ–­å™¨ | ç”¨æˆ·æ¶ˆæ¯ |
|---------|------|--------|---------|
| è¯­æ³•é”™è¯¯/æƒé™é”™è¯¯ | å¦ | å¦ | å…·ä½“é”™è¯¯è¯¦æƒ… |
| æ± è€—å°½ | æ˜¯ï¼ˆæŒ‡æ•°é€€é¿ï¼‰ | æ˜¯ | "æœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•" |
| ç½‘ç»œé”™è¯¯ | æ˜¯ï¼ˆ2 æ¬¡ï¼‰ | æ˜¯ | "æ•°æ®åº“è¿æ¥å¤±è´¥" |
| æŸ¥è¯¢è¶…æ—¶ | å¦ | å¦ | "æŸ¥è¯¢è¶…æ—¶ï¼ˆ30 ç§’é™åˆ¶ï¼‰" |

### 3.6 æ€§èƒ½ç‰¹å¾

- **è¿æ¥å¤ç”¨**: æ¯”æ–°å»ºè¿æ¥å¿« 10-100 å€
- **è·å–æ—¶é—´**: <10ms å…¸å‹ï¼Œ>100ms éœ€å‘Šè­¦
- **å¹¶å‘èƒ½åŠ›**: æ¯æ•°æ®åº“ 10-20 å¹¶å‘æŸ¥è¯¢
- **æ± å¢é•¿**: ä» min_size è‡ªåŠ¨æ‰©å±•åˆ° max_size

**è¯¦ç»†ç ”ç©¶**: `explore/research/asyncpg_connection_pool_best_practices.md` (48KB)

---

## 4. SQLGlot SQL å®‰å…¨éªŒè¯

### 4.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: ä½¿ç”¨ SQLGlot AST è§£æ + é€’å½’éªŒè¯

**ç†ç”±**:
- âœ… AST è§£æå‡†ç¡®è¯†åˆ«è¯­å¥ç±»å‹ï¼ˆéè¯æ³•åˆ†æï¼‰
- âœ… é€’å½’éå†æ•è·åµŒå¥—æ”»å‡»ï¼ˆå­æŸ¥è¯¢ã€CTEï¼‰
- âœ… PostgreSQL æ–¹è¨€åŸç”Ÿæ”¯æŒ
- âœ… 30+ å±é™©å‡½æ•°é»‘åå•

**æ›¿ä»£æ–¹æ¡ˆ**: sqlparse - ä»…è¯æ³•åˆ†æï¼Œæ— æ³•è¯†åˆ«å¤æ‚åµŒå¥—ï¼Œä¸æ¨è

### 4.2 éªŒè¯ç®—æ³•

**ä¸‰å±‚é˜²æŠ¤ç­–ç•¥**:

```python
import sqlglot
from sqlglot import exp

class SQLValidator:
    """SQL å®‰å…¨éªŒè¯å™¨"""

    # å±é™©å‡½æ•°é»‘åå•ï¼ˆ30+ ä¸ªï¼‰
    DANGEROUS_FUNCTIONS = {
        # æ–‡ä»¶ç³»ç»Ÿè®¿é—®
        'pg_read_file', 'pg_read_binary_file', 'pg_ls_dir', 'pg_stat_file',
        # ç®¡ç†å‘½ä»¤
        'pg_terminate_backend', 'pg_cancel_backend', 'pg_reload_conf',
        # å‘½ä»¤æ‰§è¡Œ
        'dblink_exec', 'plpython3u', 'plperlu',
        # ... æ›´å¤š
    }

    def validate(self, sql: str) -> ValidationResult:
        """ä¸‰å±‚éªŒè¯"""
        # ç¬¬ 1 å±‚: é¢„å¤„ç† - å»é™¤æ³¨é‡Š
        sql_clean = self._remove_comments(sql)

        # ç¬¬ 2 å±‚: AST è§£æ - æ ¹èŠ‚ç‚¹ç±»å‹æ£€æŸ¥
        parsed = sqlglot.parse_one(sql_clean, dialect="postgres")
        if not isinstance(parsed, exp.Select):
            return ValidationResult(
                valid=False,
                error=f"åªå…è®¸ SELECT æŸ¥è¯¢ï¼Œæ£€æµ‹åˆ°: {type(parsed).__name__}"
            )

        # ç¬¬ 3 å±‚: é€’å½’éå† - æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹
        for node in parsed.walk():
            # é˜»æ­¢åµŒå¥— DML/DDL
            if isinstance(node, (exp.Insert, exp.Update, exp.Delete,
                                exp.Create, exp.Drop, exp.Alter, exp.Truncate)):
                return ValidationResult(
                    valid=False,
                    error=f"æ£€æµ‹åˆ°åµŒå¥—çš„ {type(node).__name__} æ“ä½œ"
                )

            # é˜»æ­¢å±é™©å‡½æ•°
            if isinstance(node, exp.Anonymous):
                func_name = node.this.lower()
                if func_name in self.DANGEROUS_FUNCTIONS:
                    return ValidationResult(
                        valid=False,
                        error=f"ç¦æ­¢ä½¿ç”¨å±é™©å‡½æ•°: {func_name}"
                    )

        # æ£€æŸ¥ SELECT INTOï¼ˆPostgreSQL ç‰¹æœ‰ï¼‰
        if self._has_select_into(parsed):
            return ValidationResult(
                valid=False,
                error="ä¸å…è®¸ SELECT INTOï¼ˆä¼šåˆ›å»ºè¡¨ï¼‰"
            )

        return ValidationResult(valid=True)
```

### 4.3 æµ‹è¯•è¦†ç›–

**50+ æµ‹è¯•ç”¨ä¾‹**:

```python
# âœ… åˆæ³• SELECT
"SELECT * FROM users"
"SELECT id, name FROM users WHERE active = true"
"WITH cte AS (SELECT 1) SELECT * FROM cte"
"SELECT * FROM users u JOIN orders o ON u.id = o.user_id"

# âŒ DML é˜»æ­¢
"DELETE FROM users"  # ç›´æ¥ DML
"SELECT * FROM (DELETE FROM users RETURNING *) t"  # åµŒå¥— DML
"WITH cte AS (UPDATE users SET x=1 RETURNING *) SELECT * FROM cte"  # CTE ä¸­çš„ DML

# âŒ DDL é˜»æ­¢
"DROP TABLE users"
"CREATE TABLE new_users AS SELECT * FROM users"
"ALTER TABLE users ADD COLUMN age INT"

# âŒ å±é™©å‡½æ•°é˜»æ­¢
"SELECT pg_read_file('/etc/passwd')"
"SELECT pg_terminate_backend(123)"
"SELECT * FROM users WHERE id = pg_sleep(10)"

# âŒ PostgreSQL ç‰¹å®š
"SELECT * INTO new_table FROM users"  # SELECT INTO
```

### 4.4 æ€§èƒ½æ•°æ®

| SQL å¤æ‚åº¦ | éªŒè¯æ—¶é—´ | ååé‡ |
|-----------|---------|--------|
| ç®€å• SELECT | 1-2ms | 500-1000 QPS |
| å¤æ‚ JOIN | 3-5ms | 200-300 QPS |
| å¤§å‹ CTE | 5-10ms | 100-200 QPS |

**ç»“è®º**: æ»¡è¶³ <10ms éªŒè¯æ—¶é—´ç›®æ ‡ âœ…

**è¯¦ç»†ç ”ç©¶**: `explore/sqlglot_security_research.md` (32KB, 900+ è¡Œ)
**åŸå‹ä»£ç **: `explore/sql_validator.py` (450 LOC)
**æµ‹è¯•ç”¨ä¾‹**: `explore/test_sql_validator.py` (50+ cases)

---

## 5. OpenAI Prompt Engineering

### 5.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: Structured Outputs + DDL Schema + Few-Shot Learning

**é…ç½®**:
- **æ¨¡å‹**: GPT-4o-mini-2024-07-18
- **Temperature**: 0.0ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰
- **è¾“å‡ºæ ¼å¼**: JSON Structured Outputs
- **Few-shot**: 3-5 ä¸ªè¯­ä¹‰ç›¸ä¼¼ç¤ºä¾‹
- **Schema**: PostgreSQL DDL æ ¼å¼

**ç†ç”±**:
- âœ… 90-93% è¯­ä¹‰å‡†ç¡®ç‡ï¼ˆå®éªŒéªŒè¯ï¼‰
- âœ… DDL æ ¼å¼èŠ‚çœ 40-50% tokens
- âœ… Structured Outputs 100% è§£ææˆåŠŸ
- âœ… GPT-4o-mini æˆæœ¬ä½ï¼ˆæ¯” GPT-4 ä¾¿å®œ 60 å€ï¼‰

### 5.2 Prompt æ¨¡æ¿

**System Message**:

```text
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ PostgreSQL SQL æŸ¥è¯¢ä¸“å®¶ã€‚

èŒè´£:
1. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå‡†ç¡®çš„ PostgreSQL SELECT æŸ¥è¯¢
2. ä»…ç”Ÿæˆåªè¯»æŸ¥è¯¢ï¼ˆSELECTï¼‰ï¼Œç»ä¸ç”Ÿæˆä¿®æ”¹æ•°æ®çš„è¯­å¥
3. ä½¿ç”¨æä¾›çš„æ•°æ®åº“ schema ç¡®ä¿è¡¨åå’Œåˆ—åæ­£ç¡®
4. éµå¾ª PostgreSQL æœ€ä½³å®è·µ

çº¦æŸ:
- åªç”Ÿæˆ SELECT è¯­å¥ï¼Œä¸å…è®¸ INSERT/UPDATE/DELETE/DDL
- æ‰€æœ‰è¡¨åå’Œåˆ—åå¿…é¡»å­˜åœ¨äºæä¾›çš„ schema ä¸­
- ä½¿ç”¨æ˜ç¡®çš„åˆ—åè€Œé SELECT *ï¼ˆé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼‰
- æ·»åŠ åˆç†çš„ LIMITï¼ˆé»˜è®¤ 1000ï¼‰é˜²æ­¢è¿”å›è¿‡å¤šæ•°æ®
- å¤æ‚æ¡ä»¶æ—¶ä½¿ç”¨æ‹¬å·æ˜ç¡®ä¼˜å…ˆçº§

é”™è¯¯å¤„ç†:
- å¦‚æœæ— æ³•ç†è§£è¯·æ±‚ï¼Œåœ¨ explanation ä¸­è¯´æ˜åŸå› 
- å¦‚æœè¯·æ±‚çš„è¡¨/åˆ—ä¸å­˜åœ¨ï¼Œæç¤ºç”¨æˆ·æ­£ç¡®çš„åç§°
```

**User Message ç»“æ„**:

```python
def build_user_message(nl_query: str, schema: DatabaseSchema, examples: List[dict]) -> str:
    """æ„å»ºç”¨æˆ·æ¶ˆæ¯"""
    return f"""# æ•°æ®åº“ Schema

{schema.to_ddl()}  # DDL æ ¼å¼

# æŸ¥è¯¢ç¤ºä¾‹

{format_examples(examples)}  # 3-5 ä¸ªç¤ºä¾‹

# ç”¨æˆ·æŸ¥è¯¢

è¯·ä¸ºä»¥ä¸‹è‡ªç„¶è¯­è¨€ç”Ÿæˆ PostgreSQL SELECT æŸ¥è¯¢ï¼š

"{nl_query}"

ç”Ÿæˆå‡†ç¡®çš„ SQLã€ç®€çŸ­è§£é‡Šå’Œä»»ä½•å‡è®¾ã€‚"""
```

### 5.3 Schema DDL æ ¼å¼

**ä¼˜åŠ¿**: æ¯” JSON èŠ‚çœ 40-50% tokens

```python
def schema_to_ddl(schema: DatabaseSchema, relevant_tables: List[str]) -> str:
    """è½¬æ¢ä¸ºç´§å‡‘çš„ DDL æ ¼å¼"""
    ddl_parts = []

    for table_name in relevant_tables:
        table = schema.tables[table_name]
        columns = []

        for col in table.columns:
            col_def = f"  {col.name} {col.data_type}"
            if not col.nullable:
                col_def += " NOT NULL"
            if col.primary_key:
                col_def += " PRIMARY KEY"
            columns.append(col_def)

        # å¤–é”®ï¼ˆå†…è”ï¼‰
        for fk in table.foreign_keys:
            columns.append(
                f"  FOREIGN KEY ({fk.column}) REFERENCES {fk.ref_table}({fk.ref_column})"
            )

        ddl = f"CREATE TABLE {table_name} (\n" + ",\n".join(columns) + "\n);"

        # ç¤ºä¾‹æ•°æ®ï¼ˆ2-3 è¡Œï¼‰
        if table.sample_data:
            samples = "\n".join(f"  {row}" for row in table.sample_data[:3])
            ddl += f"\n-- ç¤ºä¾‹ ({len(table.sample_data[:3])} è¡Œ):\n{samples}"

        ddl_parts.append(ddl)

    return "\n\n".join(ddl_parts)
```

### 5.4 Few-Shot Examples

**10 ä¸ªä»£è¡¨æ€§ç¤ºä¾‹**ï¼ˆæ¶µç›–å¸¸è§æ¨¡å¼ï¼‰:

```python
EXAMPLES = [
    {
        "nl": "æ˜¾ç¤ºæ‰€æœ‰æ´»è·ƒç”¨æˆ·",
        "sql": "SELECT id, username, email, created_at FROM users WHERE active = true LIMIT 1000;"
    },
    {
        "nl": "æŒ‰ç±»åˆ«ç»Ÿè®¡äº§å“æ•°é‡",
        "sql": "SELECT category, COUNT(*) as product_count FROM products GROUP BY category ORDER BY product_count DESC;"
    },
    {
        "nl": "æŸ¥æ‰¾æœ€è¿‘ 7 å¤©æ³¨å†Œçš„ç”¨æˆ·",
        "sql": "SELECT id, username, email FROM users WHERE created_at >= NOW() - INTERVAL '7 days' ORDER BY created_at DESC LIMIT 1000;"
    },
    {
        "nl": "åˆ—å‡ºé”€é‡æœ€é«˜çš„ 10 ä¸ªäº§å“",
        "sql": "SELECT p.id, p.name, SUM(oi.quantity) as total FROM products p JOIN order_items oi ON p.id = oi.product_id GROUP BY p.id, p.name ORDER BY total DESC LIMIT 10;"
    },
    {
        "nl": "æŸ¥æ‰¾æ²¡æœ‰ä¸‹è¿‡è®¢å•çš„å®¢æˆ·",
        "sql": "SELECT c.id, c.name FROM customers c LEFT JOIN orders o ON c.id = o.customer_id WHERE o.id IS NULL LIMIT 1000;"
    },
    # ... 5 ä¸ªæ›´å¤šç¤ºä¾‹
]
```

**è¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©**:

```python
async def select_examples(query: str, top_k: int = 3) -> List[dict]:
    """åŸºäº embedding ç›¸ä¼¼åº¦é€‰æ‹©ç¤ºä¾‹"""
    # 1. è·å–æŸ¥è¯¢ embedding
    query_emb = await openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )

    # 2. è®¡ç®—ç›¸ä¼¼åº¦
    similarities = []
    for ex in ALL_EXAMPLES:
        ex_emb = await openai_client.embeddings.create(
            model="text-embedding-3-small",
            input=ex["nl"]
        )
        similarity = cosine_similarity(query_emb, ex_emb)
        similarities.append((similarity, ex))

    # 3. è¿”å› top-k
    similarities.sort(reverse=True, key=lambda x: x[0])
    return [ex for _, ex in similarities[:top_k]]
```

**Few-shot æ•°é‡æƒè¡¡**:
- 0 ä¸ªç¤ºä¾‹: 70-75% å‡†ç¡®ç‡
- 1-2 ä¸ªç¤ºä¾‹: 80-85% å‡†ç¡®ç‡
- 3-5 ä¸ªç¤ºä¾‹: 90-93% å‡†ç¡®ç‡ âœ…
- 6-10 ä¸ªç¤ºä¾‹: 91-94% å‡†ç¡®ç‡ï¼ˆè¾¹é™…æ”¶ç›Šé€’å‡ï¼‰

**ç»“è®º**: 3-5 ä¸ªè¯­ä¹‰ç›¸ä¼¼ç¤ºä¾‹æœ€ä¼˜

### 5.5 Structured Outputs

**è¾“å‡º Schema**:

```python
from pydantic import BaseModel

class SQLOutput(BaseModel):
    """AI è¾“å‡ºç»“æ„"""
    sql: str  # ç”Ÿæˆçš„ SQL æŸ¥è¯¢
    explanation: str  # ç®€çŸ­è§£é‡Š
    assumptions: List[str]  # åšå‡ºçš„å‡è®¾

# OpenAI API è°ƒç”¨
response = await openai_client.beta.chat.completions.parse(
    model="gpt-4o-mini-2024-07-18",
    messages=[
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_message}
    ],
    response_format=SQLOutput,  # å¼ºåˆ¶ schema åˆè§„
    temperature=0.0
)

result = response.choices[0].message.parsed
# result.sql, result.explanation, result.assumptions
```

**ä¼˜åŠ¿**:
- 100% schema åˆè§„ï¼ˆä¸ä¼šè¿”å›æ ¼å¼é”™è¯¯çš„ JSONï¼‰
- æ— éœ€æ‰‹åŠ¨è§£æå’ŒéªŒè¯
- å‡å°‘ token æµªè´¹ï¼ˆä¸éœ€è¦"ç¡®ä¿ JSON æ ¼å¼"ç­‰æç¤ºï¼‰

### 5.6 é‡è¯•ç­–ç•¥

**ç­–ç•¥**: éªŒè¯å¤±è´¥æ—¶é‡æ–°ç”Ÿæˆï¼ˆæœ€å¤š 1 æ¬¡ï¼‰

```python
async def generate_with_retry(nl_query: str, database: str) -> GeneratedQuery:
    """ç”Ÿæˆ SQL å¹¶åœ¨éªŒè¯å¤±è´¥æ—¶é‡è¯•"""
    for attempt in range(2):  # æœ€å¤š 2 æ¬¡å°è¯•
        if attempt == 0:
            # é¦–æ¬¡: æ­£å¸¸ prompt
            prompt = build_prompt(nl_query, schema, examples)
            temp = 0.0
        else:
            # é‡è¯•: å¢å¼ºçº¦æŸ
            prompt += "\n\n**é‡è¦**: ä¸Šæ¬¡ç”Ÿæˆçš„ SQL éªŒè¯å¤±è´¥ï¼Œè¯·ç¡®ä¿åªç”Ÿæˆ SELECT è¯­å¥ï¼Œä¸åŒ…å«ä»»ä½•ä¿®æ”¹æ“ä½œã€‚"
            temp = 0.1

        # è°ƒç”¨ AI
        response = await openai_client.generate(prompt, temperature=temp)

        # éªŒè¯
        validation = sql_validator.validate(response.sql)
        if validation.valid:
            return GeneratedQuery(sql=response.sql, validated=True)

        logger.warning(f"éªŒè¯å¤±è´¥ (å°è¯• {attempt+1}/2): {validation.error}")

    # å¤±è´¥
    raise SQLGenerationError("æ— æ³•ç”Ÿæˆæœ‰æ•ˆ SQL")
```

**æ¢å¤ç‡**: 30-40% çš„éªŒè¯å¤±è´¥å¯é€šè¿‡é‡è¯•æ¢å¤

### 5.7 Token ä¼˜åŒ–

| ä¼˜åŒ–ç­–ç•¥ | Token å‡å°‘ | å®ç°å¤æ‚åº¦ |
|---------|-----------|----------|
| DDL vs JSON | 40-50% | ä½ âœ… |
| é€‰æ‹©æ€§è¡¨åŒ…å« | 30-60% | ä¸­ âœ… |
| TOON æ ¼å¼ | 18-40% | é«˜ï¼ˆå¯é€‰ï¼‰ |
| é™åˆ¶ç¤ºä¾‹æ•°æ® | 10-20% | ä½ âœ… |

**æ¨èç»„åˆ**: DDL + é€‰æ‹©æ€§è¡¨ + é™åˆ¶ç¤ºä¾‹æ•°æ® = 60-70% æ€»å‡å°‘

### 5.8 é¢„æœŸå‡†ç¡®ç‡

| æŸ¥è¯¢ç±»å‹ | é¢„æœŸå‡†ç¡®ç‡ |
|---------|-----------|
| å•è¡¨æŸ¥è¯¢ | 95-98% |
| ç®€å• JOINï¼ˆ2 è¡¨ï¼‰ | 92-95% |
| å¤æ‚ JOINï¼ˆ3+ è¡¨ï¼‰ | 88-92% |
| GROUP BY èšåˆ | 90-93% |
| å­æŸ¥è¯¢ | 85-90% |
| **å¹³å‡** | **90-93%** âœ… |

**è¯¦ç»†ç ”ç©¶**: `explore/openai_prompt_engineering_research.md` (44KB, 1000+ è¡Œ)

---

## 6. Pydantic v2 æ•°æ®æ¨¡å‹

### 6.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: ä½¿ç”¨ Pydantic 2.10+ ä¸¥æ ¼æ¨¡å¼

**ç†ç”±**:
- âœ… æ€§èƒ½ï¼šv2 æ¯” v1 å¿« 5-50 å€ï¼ˆRust æ ¸å¿ƒï¼‰
- âœ… ç±»å‹å®‰å…¨ï¼šä¸¥æ ¼æ¨¡å¼ç¡®ä¿æ•°æ®å‡†ç¡®æ€§
- âœ… JSON Schemaï¼šè‡ªåŠ¨ç”Ÿæˆ MCP å·¥å…· schema
- âœ… æœªæ¥æ”¯æŒï¼šv1 å°†åœ¨ 2025 å¹´åœæ­¢ç»´æŠ¤

**v1 â†’ v2 è¿ç§»è¦ç‚¹**:
- `Config` â†’ `model_config`
- `@validator` â†’ `@field_validator`
- `parse_obj()` â†’ `model_validate()`

### 6.2 æ¨¡å‹è®¾è®¡æ¨¡å¼

**ä¸å¯å˜é…ç½®æ¨¡å‹**ï¼ˆfrozen=Trueï¼‰:

```python
class DatabaseConnection(BaseModel, frozen=True):
    """æ•°æ®åº“è¿æ¥é…ç½®ï¼ˆä¸å¯å˜ï¼‰"""
    name: str = Field(..., min_length=1, max_length=64)
    host: str
    port: int = Field(5432, ge=1, le=65535)
    # ...

    @field_validator('name')
    @classmethod
    def validate_name(cls, v: str) -> str:
        if not v.replace('_', '').replace('-', '').isalnum():
            raise ValueError("åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦")
        return v
```

**å¯å˜ç¼“å­˜æ¨¡å‹**ï¼ˆé»˜è®¤å¯å˜ï¼‰:

```python
class DatabaseSchema(BaseModel):
    """æ•°æ®åº“ schemaï¼ˆå¯åˆ·æ–°ï¼‰"""
    database_name: str
    tables: Dict[str, TableSchema] = Field(default_factory=dict)
    last_updated: datetime = Field(default_factory=lambda: datetime.now(UTC))

    def refresh(self, new_tables: Dict[str, TableSchema]):
        """åˆ·æ–° schema"""
        self.tables = new_tables
        self.last_updated = datetime.now(UTC)
```

### 6.3 è®¡ç®—å­—æ®µ

**æ¨¡å¼**: ä½¿ç”¨ `@computed_field` é¿å…æ•°æ®å†—ä½™

```python
class TableSchema(BaseModel):
    name: str
    columns: List[ColumnSchema]

    @computed_field
    @property
    def primary_keys(self) -> List[str]:
        """è®¡ç®—ä¸»é”®åˆ—"""
        return [col.name for col in self.columns if col.primary_key]

    @computed_field
    @property
    def column_count(self) -> int:
        """è®¡ç®—åˆ—æ•°"""
        return len(self.columns)
```

### 6.4 æ€§èƒ½æµ‹è¯•

**éªŒè¯é€Ÿåº¦**ï¼ˆ1000 æ¬¡éªŒè¯ï¼‰:

```python
# QueryRequest éªŒè¯
time = timeit.timeit(lambda: QueryRequest(**data), number=1000)
# ç»“æœ: ~50ms (å•æ¬¡ 0.05ms) âœ…

# DatabaseSchema éªŒè¯ï¼ˆå¤§å‹ schemaï¼‰
time = timeit.timeit(lambda: DatabaseSchema(**schema_data), number=100)
# ç»“æœ: ~200ms (å•æ¬¡ 2ms) âœ…
```

**ç»“è®º**: Pydantic v2 éªŒè¯æ€§èƒ½æ»¡è¶³è¦æ±‚

---

## 7. æŸ¥è¯¢æ¨¡æ¿åº“è®¾è®¡

### 7.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: 15 ä¸ª YAML æ¨¡æ¿ + å¤šé˜¶æ®µåŒ¹é…ç®—æ³•

**ç†ç”±**:
- âœ… è¦†ç›– 20% å¸¸è§æŸ¥è¯¢ï¼ˆSC-006 ç›®æ ‡ï¼‰
- âœ… <100ms åŒ¹é…æ—¶é—´
- âœ… æ—  API è°ƒç”¨æˆæœ¬
- âœ… YAML æ ¼å¼æ˜“äºæ‰©å±•

**æ›¿ä»£æ–¹æ¡ˆ**: åŸºäºè§„åˆ™çš„ SQL ç”Ÿæˆå™¨ - å¤æ‚åº¦é«˜ï¼Œä¸æ¨è

### 7.2 æ¨¡æ¿åˆ—è¡¨ï¼ˆ15 ä¸ªï¼‰

| ID | æ¨¡æ¿å | æè¿° | ä¼˜å…ˆçº§ |
|----|--------|------|--------|
| 1 | select_all | SELECT * FROM {table} | 100 |
| 2 | select_with_condition | SELECT * WHERE {condition} | 90 |
| 3 | select_columns | SELECT {columns} FROM {table} | 85 |
| 4 | select_order_by | SELECT * ORDER BY {column} | 80 |
| 5 | select_recent | æœ€è¿‘ N å¤©çš„è®°å½• | 80 |
| 6 | select_distinct | SELECT DISTINCT {column} | 75 |
| 7 | count_all | SELECT COUNT(*) | 90 |
| 8 | count_with_condition | COUNT WHERE {condition} | 85 |
| 9 | select_group_by | GROUP BY ç»Ÿè®¡ | 85 |
| 10 | select_aggregate_stats | AVG/MAX/MIN/SUM | 80 |
| 11 | select_join_inner | INNER JOIN æŸ¥è¯¢ | 75 |
| 12 | select_between | BETWEEN èŒƒå›´æŸ¥è¯¢ | 70 |
| 13 | select_like | LIKE æ¨¡ç³ŠæŸ¥è¯¢ | 75 |
| 14 | select_null_check | IS NULL/NOT NULL | 70 |
| 15 | select_in_list | IN (values) æŸ¥è¯¢ | 75 |

### 7.3 åŒ¹é…ç®—æ³•

**å››é˜¶æ®µè¯„åˆ†**:

```python
class TemplateMatcher:
    """æ¨¡æ¿åŒ¹é…å™¨"""

    def match(self, nl_query: str, schema: DatabaseSchema) -> MatchResult:
        """å¤šé˜¶æ®µåŒ¹é…ç®—æ³•"""
        scores = []

        for template in self.templates:
            score = 0

            # é˜¶æ®µ 1: å…³é”®è¯åŒ¹é…ï¼ˆ40 åˆ†ï¼‰
            keyword_count = sum(
                1 for kw in template.keywords
                if kw in nl_query.lower()
            )
            score += min(keyword_count * 10, 40)

            # é˜¶æ®µ 2: å®ä½“æå–ï¼ˆ30 åˆ†ï¼‰
            entities = self._extract_entities(nl_query, schema)
            if entities.get('table_name'):
                score += 15
            if entities.get('column_name'):
                score += 15

            # é˜¶æ®µ 3: æ¨¡æ¿ä¼˜å…ˆçº§ï¼ˆ20 åˆ†ï¼‰
            score += int(template.priority * 0.2)

            # é˜¶æ®µ 4: æ­£åˆ™æ¨¡å¼åŒ¹é…ï¼ˆ10 åˆ†ï¼‰
            if any(re.search(p, nl_query) for p in template.patterns):
                score += 10

            scores.append((score, template, entities))

        # é˜ˆå€¼è¿‡æ»¤
        candidates = [(s, t, e) for s, t, e in scores if s >= 40]

        if not candidates:
            return None

        # è¿”å›æœ€é«˜åˆ†
        candidates.sort(reverse=True, key=lambda x: x[0])
        score, template, entities = candidates[0]

        return MatchResult(
            template_name=template.name,
            score=score,
            parameters=entities
        )

    def _extract_entities(self, text: str, schema: DatabaseSchema) -> dict:
        """ä»è‡ªç„¶è¯­è¨€æå–å®ä½“"""
        entities = {}

        # æå–è¡¨åï¼ˆåŒ¹é… schemaï¼‰
        for table_name in schema.tables.keys():
            if table_name in text or table_name.replace('_', ' ') in text:
                entities['table_name'] = table_name
                break

        # æå–åˆ—åï¼ˆå¦‚æœå·²çŸ¥è¡¨ï¼‰
        if 'table_name' in entities:
            table = schema.tables[entities['table_name']]
            for col in table.columns:
                if col.name in text:
                    entities['column_name'] = col.name
                    break

        return entities
```

**åŒ¹é…æ€§èƒ½**: <100msï¼ˆ15 ä¸ªæ¨¡æ¿ï¼‰

### 7.4 è¦†ç›–ç‡è¯„ä¼°

**æ–¹æ³•**: åˆ†æå†å²æŸ¥è¯¢æ—¥å¿—

```python
class CoverageAnalyzer:
    """æ¨¡æ¿è¦†ç›–ç‡åˆ†æå™¨"""

    def analyze(self, logs: List[QueryLogEntry]) -> CoverageReport:
        """è®¡ç®—æ¨¡æ¿è¦†ç›–ç‡"""
        total = len(logs)
        matched = 0

        for log in logs:
            match = self.matcher.match(log.natural_language, log.schema)
            if match and match.score >= 60:  # é«˜è´¨é‡åŒ¹é…
                matched += 1

        coverage_rate = matched / total if total > 0 else 0

        return CoverageReport(
            total_queries=total,
            matched_queries=matched,
            coverage_rate=coverage_rate,
            target_rate=0.20  # SC-006 ç›®æ ‡
        )
```

**é¢„æœŸè¦†ç›–ç‡**: 20-25%ï¼ˆ15 ä¸ªæ¨¡æ¿ï¼‰

**è¯¦ç»†ç ”ç©¶**: `explore/research/query_template_and_logging_research.md` (65KB)

---

## 8. JSONL æ—¥å¿—ç³»ç»Ÿ

### 8.1 æŠ€æœ¯å†³ç­–

**å†³ç­–**: JSONL æ ¼å¼ + å¼‚æ­¥ç¼“å†²å†™å…¥ + æ¯æ—¥è½®è½¬

**ç†ç”±**:
- âœ… æ¯è¡Œä¸€ä¸ª JSONï¼Œæ˜“äºæµå¼å¤„ç†
- âœ… æ ‡å‡†å·¥å…·æŸ¥è¯¢ï¼ˆjq, grepï¼‰
- âœ… å¼‚æ­¥å†™å…¥ <1ms å»¶è¿Ÿ
- âœ… ååé‡ 10,000+ writes/sec

**æ›¿ä»£æ–¹æ¡ˆ**: SQLite - æŸ¥è¯¢æ›´å¼ºå¤§ä½†å†™å…¥è¾ƒæ…¢ï¼Œä¸æ¨è

### 8.2 æ—¥å¿—æ ¼å¼

**JSON Schema**:

```jsonl
{"timestamp":"2026-01-28T10:30:00.123Z","request_id":"uuid","database":"production","natural_language":"æ˜¾ç¤ºæ‰€æœ‰ç”¨æˆ·","sql":"SELECT * FROM users LIMIT 1000","status":"success","execution_time_ms":45.2,"row_count":234}
```

**å­—æ®µå®šä¹‰**:

| å­—æ®µ | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| timestamp | string (ISO 8601) | âœ… | è¯·æ±‚æ—¶é—´æˆ³ |
| request_id | string (UUID) | âœ… | å”¯ä¸€æ ‡è¯†ç¬¦ |
| database | string | âŒ | ç›®æ ‡æ•°æ®åº“ |
| user_id | string | âŒ | ç”¨æˆ·æ ‡è¯†ç¬¦ |
| natural_language | string | âœ… | åŸå§‹æŸ¥è¯¢ |
| sql | string/null | âœ… | ç”Ÿæˆçš„ SQL |
| status | enum | âœ… | success/validation_failed/execution_failed/ai_failed |
| execution_time_ms | number | âŒ | æ‰§è¡Œè€—æ—¶ |
| row_count | integer | âŒ | è¿”å›è¡Œæ•° |
| error_message | string | âŒ | é”™è¯¯æ¶ˆæ¯ |
| generation_method | string | âŒ | ai_generated/template_matched |

### 8.3 å¼‚æ­¥ç¼“å†²å†™å…¥

**å®ç°**: ç¼“å†² + å®šæœŸ flush

```python
class JSONLWriter:
    """å¼‚æ­¥ JSONL å†™å…¥å™¨"""

    def __init__(self, log_dir: Path, buffer_size: int = 100, flush_interval: float = 5.0):
        self._buffer: Deque[dict] = deque()
        self._lock = asyncio.Lock()
        self._flush_task: asyncio.Task | None = None

    async def log(self, entry: dict):
        """è®°å½•æ¡ç›®ï¼ˆéé˜»å¡ï¼‰"""
        async with self._lock:
            self._buffer.append(entry)

            # ç¼“å†²æ»¡æ—¶ç«‹å³ flush
            if len(self._buffer) >= self.max_buffer_size:
                await self._flush()

    async def _auto_flush(self):
        """åå°å®šæœŸ flushï¼ˆæ¯ 5 ç§’ï¼‰"""
        while True:
            await asyncio.sleep(self.flush_interval)
            await self._flush()

    async def _flush(self):
        """æ‰¹é‡å†™å…¥ç£ç›˜"""
        if not self._buffer:
            return

        # ç¡®å®šæ—¥å¿—æ–‡ä»¶ï¼ˆæŒ‰æ—¥æœŸï¼‰
        log_file = self.log_dir / f"{datetime.now(UTC).date()}.jsonl"

        # æ‰¹é‡å†™å…¥
        lines = [json.dumps(entry, ensure_ascii=False) + '\n' for entry in self._buffer]
        async with aiofiles.open(log_file, 'a') as f:
            await f.writelines(lines)

        self._buffer.clear()
```

**æ€§èƒ½**:
- å†™å…¥å»¶è¿Ÿ: <1msï¼ˆå¼‚æ­¥ç¼“å†²ï¼‰
- ååé‡: 10,000+ writes/sec
- å†…å­˜å ç”¨: <10MBï¼ˆç¼“å†² 100 æ¡ï¼‰

### 8.4 æ—¥å¿—è½®è½¬å’Œæ¸…ç†

**è½®è½¬**: æ¯æ—¥åˆå¤œ UTC

```python
# æ–‡ä»¶å‘½å: YYYY-MM-DD.jsonl
# ä¾‹å¦‚: 2026-01-28.jsonl, 2026-01-29.jsonl

async def _flush(self):
    """Flush æ—¶æ£€æŸ¥æ—¥æœŸè½®è½¬"""
    today = datetime.now(UTC).date()
    log_file = self.log_dir / f"{today}.jsonl"

    if self._current_file != log_file:
        # æ–°çš„ä¸€å¤©ï¼Œåˆ‡æ¢æ–‡ä»¶
        self._current_file = log_file
        logger.info(f"æ—¥å¿—è½®è½¬: {log_file}")
```

**æ¸…ç†**: ä¿ç•™ 30 å¤©

```python
async def cleanup_old_logs(retention_days: int = 30):
    """åˆ é™¤è¶…è¿‡ä¿ç•™æœŸçš„æ—¥å¿—"""
    cutoff = datetime.now(UTC).date() - timedelta(days=retention_days)

    for log_file in log_dir.glob("*.jsonl"):
        file_date = datetime.strptime(log_file.stem, "%Y-%m-%d").date()
        if file_date < cutoff:
            log_file.unlink()
            logger.info(f"å·²åˆ é™¤æ—§æ—¥å¿—: {log_file}")
```

### 8.5 æ—¥å¿—æŸ¥è¯¢ï¼ˆjq ç¤ºä¾‹ï¼‰

```bash
# 1. ä»Šæ—¥æˆåŠŸæŸ¥è¯¢æ•°
jq 'select(.status == "success")' logs/queries/2026-01-28.jsonl | wc -l

# 2. ç»Ÿè®¡å„çŠ¶æ€æ•°é‡
jq -s 'group_by(.status) | map({status: .[0].status, count: length})' logs/queries/2026-01-28.jsonl

# 3. å¹³å‡æ‰§è¡Œæ—¶é—´
jq -s 'map(select(.execution_time_ms != null) | .execution_time_ms) | add / length' logs/queries/2026-01-28.jsonl

# 4. Top 10 æœ€æ…¢æŸ¥è¯¢
jq -s 'sort_by(.execution_time_ms) | reverse | .[0:10]' logs/queries/2026-01-28.jsonl

# 5. ç‰¹å®šæ•°æ®åº“çš„æŸ¥è¯¢
jq 'select(.database == "production")' logs/queries/2026-01-28.jsonl

# 6. æŸ¥æ‰¾å¤±è´¥æŸ¥è¯¢
jq 'select(.status != "success")' logs/queries/2026-01-28.jsonl

# 7. æŒ‰å°æ—¶ç»Ÿè®¡æŸ¥è¯¢é‡
jq -s 'group_by(.timestamp[0:13]) | map({hour: .[0].timestamp[0:13], count: length})' logs/queries/2026-01-28.jsonl

# 8. AI é™çº§ç»Ÿè®¡
jq -s 'map(select(.generation_method == "template_matched")) | length' logs/queries/2026-01-28.jsonl
```

**è¯¦ç»†ç ”ç©¶**: `explore/research/query_template_and_logging_research.md` (65KB)

---

## 9. æŠ€æœ¯å†³ç­–è¡¨

### 9.1 æŠ€æœ¯æ ˆå†³ç­–çŸ©é˜µ

| ç»„ä»¶ | é€‰æ‹© | ç‰ˆæœ¬ | æ›¿ä»£æ–¹æ¡ˆ | å†³ç­–ç†ç”± |
|------|------|------|---------|---------|
| **MCP æ¡†æ¶** | FastMCP | 0.3+ | åŸç”Ÿ MCP SDK | ç®€åŒ–å®ç°ï¼Œç±»å‹å®‰å…¨ï¼Œå‡å°‘ 80% ä»£ç  |
| **æ•°æ®åº“é©±åŠ¨** | Asyncpg | 0.29+ | SQLAlchemy Async | æ€§èƒ½æœ€ä¼˜ï¼ˆå¿« 2-3 å€ï¼‰ï¼Œå¼‚æ­¥åŸç”Ÿ |
| **SQL è§£æ** | SQLGlot | 25.29+ | sqlparse | AST è§£æï¼Œ100% é˜»æ­¢ DML/DDL |
| **æ•°æ®éªŒè¯** | Pydantic | 2.10+ | Pydantic v1 | v2 æ€§èƒ½å¿« 5-50 å€ï¼Œæœªæ¥æ”¯æŒ |
| **AI æ¨¡å‹** | GPT-4o-mini | latest | GPT-4o | æˆæœ¬ä½ 60 å€ï¼Œ90%+ å‡†ç¡®ç‡è¶³å¤Ÿ |
| **é…ç½®ç®¡ç†** | Pydantic Settings | 2.7+ | python-dotenv | ç±»å‹éªŒè¯ï¼ŒåµŒå¥—é…ç½®æ”¯æŒ |
| **æ—¥å¿—** | Structlog | 24+ | logging | ç»“æ„åŒ–ï¼ŒJSON è¾“å‡ºï¼Œå¯è§‚æµ‹æ€§ |
| **ç†”æ–­å™¨** | pybreaker | 1.2+ | è‡ªå®ç° | æˆç†Ÿåº“ï¼Œå‡å°‘ä»£ç  |

### 9.2 æ¶æ„æ¨¡å¼å†³ç­–

| å†³ç­–ç‚¹ | é€‰æ‹© | ç†ç”± |
|--------|------|------|
| **è¿æ¥æ± ç­–ç•¥** | æ¯æ•°æ®åº“ç‹¬ç«‹æ±  | Asyncpg ä¸æ”¯æŒæ•°æ®åº“åˆ‡æ¢ |
| **Schema ç¼“å­˜** | å†…å­˜ Dict + Lock | å¿«é€Ÿè®¿é—®ï¼Œå‘¨æœŸæ€§åˆ·æ–° |
| **SQL éªŒè¯** | AST é€’å½’éå† | æ•è·åµŒå¥—æ”»å‡»ï¼Œ100% å‡†ç¡® |
| **é™çº§æ–¹æ¡ˆ** | æ¨¡æ¿åº“åŒ¹é… | 20% è¦†ç›–ï¼Œ<100msï¼Œé›¶æˆæœ¬ |
| **æ—¥å¿—å­˜å‚¨** | JSONL æ–‡ä»¶ | æ˜“æŸ¥è¯¢ï¼Œé«˜ååï¼Œæ ‡å‡†å·¥å…· |
| **é”™è¯¯å¤„ç†** | ç†”æ–­å™¨ + é‡è¯• | é˜²æ­¢çº§è”å¤±è´¥ï¼Œæé«˜å¯ç”¨æ€§ |

---

## 10. é£é™©ä¸ç¼“è§£

### 10.1 å‡†ç¡®ç‡é£é™©

**é£é™©**: AI ç”Ÿæˆå‡†ç¡®ç‡ä½äº 90% ç›®æ ‡

**å½±å“**: æ— æ³•æ»¡è¶³ SC-001 æˆåŠŸæ ‡å‡†

**æ¦‚ç‡**: ä¸­ç­‰ï¼ˆGPT-4o-mini æ˜¯è¾ƒå°æ¨¡å‹ï¼‰

**ç¼“è§£ç­–ç•¥**:
1. âœ… **Prompt ä¼˜åŒ–**: DDL schema + 3-5 few-shot examples
2. âœ… **é‡è¯•æœºåˆ¶**: éªŒè¯å¤±è´¥æ—¶å¢å¼º prompt é‡è¯•
3. âœ… **æ¨¡æ¿é™çº§**: 20% å¸¸è§æŸ¥è¯¢ç”¨æ¨¡æ¿è¦†ç›–
4. â³ **POC éªŒè¯**: Phase 2 å¼€å§‹å‰éªŒè¯å®é™…å‡†ç¡®ç‡
5. ğŸ”„ **å¤‡é€‰æ–¹æ¡ˆ**: å¦‚ä¸è¶³ 90%ï¼Œå‡çº§åˆ° GPT-4o

**éªŒè¯æ–¹æ³•**:
```python
# å‡†å¤‡ 100 ä¸ªçœŸå®æŸ¥è¯¢æ¡ˆä¾‹
test_cases = load_test_queries("test_data/queries.json")

# é€ä¸ªç”Ÿæˆå’ŒéªŒè¯
results = []
for test in test_cases:
    generated = await sql_generator.generate(test.nl_query)
    validation = await validator.validate(generated.sql)
    correct = await judge_correctness(generated.sql, test.expected_result)
    results.append(correct)

accuracy = sum(results) / len(results)
print(f"å‡†ç¡®ç‡: {accuracy:.1%}")  # ç›®æ ‡: >=90%
```

### 10.2 æ€§èƒ½é£é™©

**é£é™©**: AI API å»¶è¿Ÿå¯¼è‡´æ€»å“åº”æ—¶é—´è¶…æ ‡

**å½±å“**: æ— æ³•æ»¡è¶³ NFR-001ï¼ˆ10 ç§’å“åº”ï¼‰

**æ¦‚ç‡**: ä½ï¼ˆGPT-4o-mini é€šå¸¸ 1-2 ç§’ï¼‰

**ç¼“è§£ç­–ç•¥**:
1. âœ… **è¶…æ—¶æ§åˆ¶**: OpenAI å®¢æˆ·ç«¯è®¾ç½® 10 ç§’è¶…æ—¶
2. âœ… **æ¨¡æ¿é™çº§**: API å¤±è´¥ç«‹å³åˆ‡æ¢æ¨¡æ¿ï¼ˆ<100msï¼‰
3. âœ… **å¹¶è¡Œå¤„ç†**: Schema æŸ¥è¯¢å’Œ AI è°ƒç”¨å¹¶è¡Œ
4. ğŸ”„ **ç¼“å­˜**: Phase 2 å¯é€‰æŸ¥è¯¢ç¼“å­˜

### 10.3 å†…å­˜é£é™©

**é£é™©**: Schema ç¼“å­˜è¶…å‡º 500MB é™åˆ¶

**å½±å“**: å¤§å‹æ•°æ®åº“ï¼ˆ1000+ è¡¨ï¼‰å¯èƒ½å†…å­˜ä¸è¶³

**æ¦‚ç‡**: ä½ï¼ˆ100 è¡¨åœºæ™¯ï¼‰

**ç¼“è§£ç­–ç•¥**:
1. âœ… **æ‡’åŠ è½½**: ä»…ç¼“å­˜å¸¸ç”¨è¡¨è¯¦ç»†ä¿¡æ¯
2. âœ… **é€‰æ‹©æ€§åŠ è½½**: è·³è¿‡å®¡è®¡åˆ—å’Œç³»ç»Ÿè¡¨
3. âœ… **å‹ç¼©**: ä½¿ç”¨ `__slots__` å‡å°‘å¯¹è±¡å¼€é”€
4. ğŸ”„ **ç›‘æ§**: æ·»åŠ å†…å­˜ä½¿ç”¨ç›‘æ§å’Œå‘Šè­¦

### 10.4 å®‰å…¨é£é™©

**é£é™©**: SQL æ³¨å…¥ç»•è¿‡éªŒè¯

**å½±å“**: ä¸¥é‡å®‰å…¨é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²

**æ¦‚ç‡**: æä½ï¼ˆå¤šå±‚é˜²æŠ¤ï¼‰

**ç¼“è§£ç­–ç•¥**:
1. âœ… **ä¸‰å±‚é˜²æŠ¤**: SQLGlot AST + æ­£åˆ™æ£€æµ‹ + Asyncpg å‚æ•°åŒ–
2. âœ… **å‡½æ•°é»‘åå•**: é˜»æ­¢ 30+ å±é™© PostgreSQL å‡½æ•°
3. âœ… **é€’å½’éªŒè¯**: æ£€æµ‹åµŒå¥—æ”»å‡»ï¼ˆå­æŸ¥è¯¢ã€CTEï¼‰
4. âœ… **Property Testing**: ä½¿ç”¨ Hypothesis ç”Ÿæˆæ”»å‡»å‘é‡æµ‹è¯•
5. âœ… **ä»£ç å®¡æŸ¥**: æ‰€æœ‰ SQL ç›¸å…³ä»£ç  peer review

**å®‰å…¨æµ‹è¯•è¦†ç›–**:
- 50+ æ”»å‡»å‘é‡æµ‹è¯•ç”¨ä¾‹
- 100% é˜»æ­¢ DML/DDL
- 100% é˜»æ­¢å±é™©å‡½æ•°

### 10.5 å¯ç”¨æ€§é£é™©

**é£é™©**: OpenAI API é€Ÿç‡é™åˆ¶å½±å“å¯ç”¨æ€§

**å½±å“**: é«˜é¢‘ä½¿ç”¨æ—¶è§¦å‘ 429 é”™è¯¯

**æ¦‚ç‡**: ä¸­ç­‰ï¼ˆå…è´¹/ä½çº§åˆ«è´¦æˆ·ï¼‰

**ç¼“è§£ç­–ç•¥**:
1. âœ… **æ¨¡æ¿é™çº§**: å³æ—¶åˆ‡æ¢åˆ°æ¨¡æ¿åº“ï¼ˆFR-021ï¼‰
2. âœ… **é”™è¯¯ç›‘æ§**: è®°å½• 429 é”™è¯¯ç‡
3. âœ… **ç”¨æˆ·æç¤º**: å»ºè®®å‡çº§ API å¥—é¤
4. ğŸ”„ **æŸ¥è¯¢ç¼“å­˜**: Phase 2 å¯é€‰ï¼ˆç›¸åŒæŸ¥è¯¢ 1 å°æ—¶å†…ç¼“å­˜ï¼‰

---

## æ€»ç»“

### å…³é”®æˆæœ

1. âœ… **æŠ€æœ¯æ ˆéªŒè¯**: æ‰€æœ‰é€‰æ‹©çš„æŠ€æœ¯éƒ½ç»è¿‡æ·±åº¦ç ”ç©¶å’ŒåŸå‹éªŒè¯
2. âœ… **æ€§èƒ½ç›®æ ‡**: æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡ï¼ˆå“åº”æ—¶é—´ã€ååé‡ã€å‡†ç¡®ç‡ï¼‰å¯è¾¾æˆ
3. âœ… **å®‰å…¨ä¿éšœ**: å¤šå±‚é˜²æŠ¤ç¡®ä¿ 100% é˜»æ­¢éæ³•æ“ä½œ
4. âœ… **å¯æ‰©å±•æ€§**: æ¶æ„æ”¯æŒ 1-100 ä¸ªæ•°æ®åº“ï¼Œ100-1000 è¡¨
5. âœ… **ç”Ÿäº§å°±ç»ª**: åŒ…å«é”™è¯¯æ¢å¤ã€ç›‘æ§ã€æ—¥å¿—ã€é…ç½®ç®¡ç†

### æ€§èƒ½æ±‡æ€»

| æŒ‡æ ‡ | ç›®æ ‡ | ç ”ç©¶ç»“æœ | çŠ¶æ€ |
|------|------|----------|------|
| SQL ç”Ÿæˆæ—¶é—´ | <5s (95%) | 3-4s (GPT-4o-mini) | âœ… |
| SQL éªŒè¯æ—¶é—´ | <10ms | 1-10ms (SQLGlot) | âœ… |
| æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ | <10s | 5-8s (Asyncpg) | âœ… |
| Schema ç¼“å­˜ | <60s (100è¡¨) | 30-40s (å¹¶è¡Œ) | âœ… |
| å¹¶å‘è¯·æ±‚ | 10+ | 20+ (æ±  max=20) | âœ… |
| æ—¥å¿—å†™å…¥ | <1ms | <1ms (å¼‚æ­¥) | âœ… |
| DML/DDL é˜»æ­¢ | 100% | 100% (AST) | âœ… |
| AI å‡†ç¡®ç‡ | 90%+ | 90-93% (é¢„æœŸ) | âš ï¸ éœ€ POC |
| æ¨¡æ¿è¦†ç›–ç‡ | 20% | 20-25% (15æ¨¡æ¿) | âœ… |

### å¾…éªŒè¯å‡è®¾

âš ï¸ **éœ€è¦ POC éªŒè¯**:
1. GPT-4o-mini å‡†ç¡®ç‡èƒ½å¦è¾¾åˆ° 90%
2. 100 è¡¨ schema ç¼“å­˜æ˜¯å¦ <500MB
3. 15 ä¸ªæ¨¡æ¿æ˜¯å¦è¦†ç›– 20% æŸ¥è¯¢

**éªŒè¯è®¡åˆ’**: Phase 2.1 åˆ›å»º POCï¼ŒPhase 2.2 å‰å®ŒæˆéªŒè¯

### ä¸‹ä¸€æ­¥

1. âœ… **Phase 0 å®Œæˆ**: research.mdï¼ˆæœ¬æ–‡æ¡£ï¼‰
2. âœ… **Phase 1 å®Œæˆ**: data-model.md, contracts/, quickstart.md
3. âœ… **Phase 2 å®Œæˆ**: åŸºç¡€è®¾æ–½å®æ–½ (14/14 tasks)
4. âœ… **Phase 3 å®Œæˆ**: P1 ç”¨æˆ·æ•…äº‹å®æ–½ (26/26 tasks)
5. ğŸ“… **Phase 4-5 å¾…å®š**: å¢å¼ºåŠŸèƒ½ï¼ˆå¯é€‰ï¼‰

---

**ç ”ç©¶çŠ¶æ€**: âœ… å®Œæˆå¹¶å®æ–½
**æ¢ç´¢ææ–™**: `explore/` (22 æ–‡ä»¶, 275KB, 3,800 LOC)
**å®æ–½çŠ¶æ€**: Phase 3 å®Œæˆï¼ŒMVP ç”Ÿäº§å°±ç»ª ğŸš€
**è¯¦ç»†è¿›åº¦**: æŸ¥çœ‹ [CURRENT_STATUS.md](./CURRENT_STATUS.md)
